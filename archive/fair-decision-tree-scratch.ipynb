{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': fetch_lawschool_gpa will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.sklearn.datasets import fetch_adult\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Node:\n",
    "#     '''\n",
    "#     Helper class which implements a single tree node.\n",
    "#     '''\n",
    "#     def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "#         self.feature = feature\n",
    "#         self.threshold = threshold\n",
    "#         self.data_left = data_left\n",
    "#         self.data_right = data_right\n",
    "#         self.gain = gain\n",
    "#         self.value = value\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, leaf=None, feature=None, threshold=None, greater_node=None, less_node=None, gain=None, value=None):\n",
    "        self.leaf = leaf\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.greater_node = greater_node\n",
    "        self.less_node = less_node\n",
    "        self.data = []\n",
    "        self.height = None\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "    \n",
    "    def add_greater_node(self, greater_node):\n",
    "        self.greater_node = greater_node\n",
    "\n",
    "    def add_less_node(self, less_node):\n",
    "        self.less_node = less_node\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.leaf != None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_leaf():\n",
    "            return \"{class=\"+str(self.leaf)+\"}\"\n",
    "        return \"{<=\"+str(self.threshold)+\":\"+str(self.less_node)+\",>\"+str(self.threshold)+\":\"+str(self.greater_node)+\"}\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.is_leaf():\n",
    "            return \"{class=\"+str(self.leaf)+\"}\"\n",
    "        return \"{<=\"+str(self.threshold)+\":\"+str(self.less_node)+\",>\"+str(self.threshold)+\":\"+str(self.greater_node)+\"}\"\n",
    "    \n",
    "    def export_json(self):\n",
    "        if self.is_leaf():\n",
    "            return {\"class\":self.leaf}\n",
    "        return {str(self.feature)+\"<=\"+str(self.threshold):self.less_node.export_json(), str(self.feature)+\">\"+str(self.threshold):self.greater_node.export_json()}\n",
    "    \n",
    "    def add_data(self, data):\n",
    "        if not self.is_leaf():\n",
    "            if data[self.feature] <= self.threshold:\n",
    "                leaf = self.less_node.add_data(data)\n",
    "            else:\n",
    "                leaf = self.greater_node.add_data(data)\n",
    "            data = np.append(data, leaf)\n",
    "            self.data.append(data)\n",
    "            return leaf\n",
    "        else:\n",
    "            data = np.append(data, self.leaf)\n",
    "            self.data.append(data)\n",
    "            return self.leaf\n",
    "\n",
    "    \n",
    "    def get_leafs(self):\n",
    "        if self.is_leaf():\n",
    "            return [self]\n",
    "        leafs = []\n",
    "        leafs.extend(self.less_node.get_leafs())\n",
    "        leafs.extend(self.greater_node.get_leafs())\n",
    "        return leafs\n",
    "\n",
    "    def get_accuracy(self, index=None, value=None):\n",
    "        if self.is_leaf():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in self.data:\n",
    "                if index==None:\n",
    "                    correct+=data[-1] == self.leaf\n",
    "                    total+=1\n",
    "                elif data[index] == value:\n",
    "                    correct+=data[-1] == self.leaf\n",
    "                    total+=1\n",
    "            if total == 0:\n",
    "                print('no matches for index:',index,'value:',value)\n",
    "                return -1\n",
    "            return correct/total\n",
    "        \n",
    "        print(\"not a leaf\")\n",
    "        return -1\n",
    "\n",
    "    def get_prediction(self, data):\n",
    "        if self.is_leaf():\n",
    "            return self.leaf\n",
    "        else:\n",
    "            if data[self.feature] <= self.threshold:\n",
    "                return self.less_node.get_prediction(data)\n",
    "            else:\n",
    "                return self.greater_node.get_prediction(data)\n",
    "    \n",
    "    def relabel(self):\n",
    "        if self.is_leaf():\n",
    "            self.leaf = [1.0,0.0][round(self.leaf)]\n",
    "        else:\n",
    "            print(\"not a leaf\")\n",
    "\n",
    "    def flip_nodes(self):\n",
    "        if self.is_leaf():\n",
    "            print(\"is leaf\")\n",
    "        else:\n",
    "            temp = self.greater_node\n",
    "            self.greater_node = self.less_node\n",
    "            self.less_node = temp\n",
    "            self.reset_data()\n",
    "\n",
    "    def get_height(self):\n",
    "        if self.height != None:\n",
    "            return self.height\n",
    "        elif self.is_leaf():\n",
    "            return 0\n",
    "        else:\n",
    "            return max(self.less_node.get_height(), self.greater_node.get_height()) + 1\n",
    "\n",
    "    def get_layer(self, height):\n",
    "        if self.get_height() == height:\n",
    "            return [self]\n",
    "        elif self.get_height() < height:\n",
    "            return []\n",
    "        else:\n",
    "            temp = self.less_node.get_layer(height)\n",
    "            temp.extend(self.greater_node.get_layer(height))\n",
    "            return temp\n",
    "\n",
    "    def reset_data(self):\n",
    "        if self.is_leaf():\n",
    "            self.data = []\n",
    "        else:\n",
    "            self.less_node.reset_data()\n",
    "            self.greater_node.reset_data()\n",
    "\n",
    "    def copy(self):\n",
    "        if self.is_leaf():\n",
    "            copy = Node(self.leaf, self.feature, self.threshold)\n",
    "            return copy\n",
    "        else:\n",
    "            copy = Node(self.leaf, self.feature, self.threshold)\n",
    "            if self.less_node != None:\n",
    "                copy.add_greater_node(self.greater_node.copy())\n",
    "            if self.greater_node != None:\n",
    "                copy.add_less_node(self.less_node.copy())\n",
    "            return copy\n",
    "\n",
    "    def relabel_acc(self):\n",
    "        if self.is_leaf():\n",
    "            if self.data != []:\n",
    "                self.leaf = round(np.average(np.array(self.data)[:,-1]))\n",
    "        else:\n",
    "            if self.less_node != None:\n",
    "                self.less_node.relabel_acc()\n",
    "            if self.greater_node != None:\n",
    "                self.greater_node.relabel_acc()\n",
    "\n",
    "    def discrimination(self, discriminatory_index):\n",
    "        return discrimination(self.data, discriminatory_index)\n",
    "\n",
    "    def accuracy(self):\n",
    "        return accuracy(self.data)\n",
    "\n",
    "    def get_bad_nodes(self, comp, discriminatory_index):\n",
    "        if comp == None:\n",
    "            comp = self.discrimination(discriminatory_index)\n",
    "        if (self.less_node.is_leaf() and self.discrimination(discriminatory_index) < comp) or (not self.less_node.is_leaf() and self.less_node.discrimination(discriminatory_index) < comp):\n",
    "            if (self.greater_node.is_leaf() and self.discrimination(discriminatory_index) < comp) or (not self.greater_node.is_leaf() and self.greater_node.discrimination(discriminatory_index) < comp):\n",
    "                # RETRAIN sub tree\n",
    "                print(\"retrain node:\\n\"+str(self))\n",
    "                print(\"discrimination:\", self.discrimination(discriminatory_index))\n",
    "                return\n",
    "        if not self.less_node.is_leaf():\n",
    "            self.less_node.get_bad_nodes(comp, discriminatory_index)\n",
    "        if not self.greater_node.is_leaf():\n",
    "            self.greater_node.get_bad_nodes(comp, discriminatory_index)\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeFair:\n",
    "    '''\n",
    "    Class which implements a decision tree classifier algorithm.\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        '''\n",
    "        Helper function, calculates entropy from an array of integer values.\n",
    "        \n",
    "        :param s: list\n",
    "        :return: float, entropy value\n",
    "        '''\n",
    "        # Convert to integers to avoid runtime errors\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
    "        # Probabilities of each class label\n",
    "        percentages = counts / len(s)\n",
    "\n",
    "        # Caclulate entropy\n",
    "        entropy = 0\n",
    "        for pct in percentages:\n",
    "            if pct > 0:\n",
    "                entropy += pct * np.log2(pct)\n",
    "        return -entropy\n",
    "    \n",
    "    def _statisticalParity(self, left_child, right_child, left_pred, right_pred, protectedIndex):\n",
    "        left_child = np.insert(left_child, -1, left_pred, axis=1)\n",
    "        right_child = np.insert(right_child, -1, right_pred, axis=1)\n",
    "        data = np.append(left_child, right_child, axis=0)\n",
    "        protectedClass = data[np.where(data[:,protectedIndex]==0)]\n",
    "        elseClass = data[np.where(data[:,protectedIndex]!=0)]\n",
    "    \n",
    "        if len(protectedClass) == 0 or len(elseClass) == 0:\n",
    "            # print(\"protectedClass or elseClass is empty\")\n",
    "            return -1\n",
    "        else:\n",
    "            protectedProb = np.count_nonzero(protectedClass[:,-1]) / len(protectedClass)\n",
    "            elseProb = np.count_nonzero(elseClass[:,-1]) / len(elseClass)\n",
    "    \n",
    "        statParity = protectedProb-elseProb\n",
    "\n",
    "        protectedClass = protectedClass[np.where(protectedClass[:,-1]==protectedClass[:,-2])]\n",
    "        elseClass = elseClass[np.where(elseClass[:,-1]==elseClass[:,-2])]\n",
    "    \n",
    "        if len(protectedClass) == 0 or len(elseClass) == 0:\n",
    "            # print(\"protectedClass or elseClass is empty\")\n",
    "            return -1\n",
    "        else:\n",
    "            protectedProb = np.count_nonzero(protectedClass[:,-1]) / len(protectedClass)\n",
    "            elseProb = np.count_nonzero(elseClass[:,-1]) / len(elseClass)\n",
    "\n",
    "        equalOdds = protectedProb-elseProb\n",
    "        # if left_pred!=right_pred:\n",
    "        #     print('disc', protectedClass[:,[protectedIndex, -2, -1]], elseClass[:,[protectedIndex, -2, -1]])\n",
    "\n",
    "        return (equalOdds + statParity)/2\n",
    "\n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        '''\n",
    "        Helper function, calculates information gain from a parent and two child nodes.\n",
    "        \n",
    "        :param parent: list, the parent node\n",
    "        :param left_child: list, left child of a parent\n",
    "        :param right_child: list, right child of a parent\n",
    "        :return: float, information gain\n",
    "        '''\n",
    "        num_left = len(left_child) / len(parent)\n",
    "        num_right = len(right_child) / len(parent)\n",
    "        \n",
    "        # One-liner which implements the previously discussed formula\n",
    "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
    "    \n",
    "    def _best_split(self, X, y, protected_index):\n",
    "        '''\n",
    "        Helper function, calculates the best split for given features and target\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: dict\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_gain = -float('inf')\n",
    "        best_info_gain = 0\n",
    "        best_discrimination = 0\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # For every dataset feature\n",
    "        for f_idx in range(n_cols):\n",
    "            X_curr = X[:, f_idx]\n",
    "            # For every unique value of that feature\n",
    "            for threshold in np.unique(X_curr):\n",
    "                # Construct a dataset and split it to the left and right parts\n",
    "                # Left part includes records lower or equal to the threshold\n",
    "                # Right part includes records higher than the threshold\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "\n",
    "                # Do the calculation only if there's data in both subsets\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "                    # Obtain the value of the target variable for subsets\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "\n",
    "                    pred_left = np.average(y_left) > 0.5\n",
    "                    pred_right = np.average(y_right) > 0.5\n",
    "\n",
    "\n",
    "                    sensitivity = df[:, protected_index]\n",
    "                    sensitivity_left = df_left[:, protected_index]\n",
    "                    sensitivity_right = df_right[:, protected_index]\n",
    "\n",
    "                    # Caclulate the information gain and save the split parameters\n",
    "                    # if the current split if better then the previous best\n",
    "                    info_gain = self._information_gain(y, y_left, y_right)\n",
    "                    # sensitivity_gain = self._information_gain(sensitivity, sensitivity_left, sensitivity_right)\n",
    "                    discrimination = self._statisticalParity(df_left, df_right, pred_left, pred_right, protected_index)\n",
    "                    gain = (info_gain + discrimination + 1)/2\n",
    "                    # print(info_gain, discrimination, gain)\n",
    "                    if gain > best_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_gain = gain\n",
    "                        best_info_gain = info_gain\n",
    "                        best_discrimination = discrimination\n",
    "                        # print('temp:',best_split['feature_index'],int(best_split['threshold']),'\\t', best_gain, best_info_gain, best_discrimination, pred_left==pred_right)\n",
    "        # print(best_split['feature_index'],int(best_split['threshold']),'\\t', best_gain, best_info_gain, best_discrimination)\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, sensitive_index, depth=0):\n",
    "        '''\n",
    "        Helper recursive function, used to build a decision tree from the input data.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :param depth: current depth of a tree, used as a stopping criteria\n",
    "        :return: Node\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best = self._best_split(X, y, sensitive_index)\n",
    "            # If the split isn't pure\n",
    "            if best['gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    sensitive_index=sensitive_index,\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    sensitive_index=sensitive_index,\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    less_node=left, \n",
    "                    greater_node=right, \n",
    "                    gain=best['gain']\n",
    "                )\n",
    "        # Leaf node - value is the most common target value \n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y, sensitive_index):\n",
    "        '''\n",
    "        Function used to train a decision tree classifier model.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Call a recursive function to build the tree\n",
    "        self.root = self._build(X, y, sensitive_index)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Helper recursive function, used to predict a single instance (tree traversal).\n",
    "        \n",
    "        :param x: single observation\n",
    "        :param tree: built tree\n",
    "        :return: float, predicted class\n",
    "        '''\n",
    "        # Leaf node\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # Go to the left\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.less_node)\n",
    "        \n",
    "        # Go to the right\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.greater_node)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function used to classify new instances.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :return: np.array, predicted classes\n",
    "        '''\n",
    "        # Call the _predict() function for every observation\n",
    "        return [self._predict(x, self.root) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_statParity_equalOdds(data, protectedIndex):\n",
    "    if type(data) == list:\n",
    "        data = np.array(data)\n",
    "    protectedClass = data[np.where(data[:,protectedIndex]==0)]\n",
    "    elseClass = data[np.where(data[:,protectedIndex]!=0)]\n",
    " \n",
    "    if len(protectedClass) == 0 or len(elseClass) == 0:\n",
    "        print(\"protectedClass or elseClass is empty\")\n",
    "        return 0\n",
    "    else:\n",
    "        protectedProb = np.count_nonzero(protectedClass[:,-1]) / len(protectedClass)\n",
    "        elseProb = np.count_nonzero(elseClass[:,-1]) / len(elseClass)\n",
    " \n",
    "    statParity = protectedProb-elseProb\n",
    "\n",
    "    protectedClass = protectedClass[np.where(protectedClass[:,-1]==protectedClass[:,-2])]\n",
    "    elseClass = elseClass[np.where(elseClass[:,-1]==elseClass[:,-2])]\n",
    " \n",
    "    if len(protectedClass) == 0 or len(elseClass) == 0:\n",
    "        print(\"protectedClass or elseClass is empty\")\n",
    "        return 0\n",
    "    else:\n",
    "        protectedProb = np.count_nonzero(protectedClass[:,-1]) / len(protectedClass)\n",
    "        elseProb = np.count_nonzero(elseClass[:,-1]) / len(elseClass)\n",
    "\n",
    "    equalOdds = protectedProb-elseProb\n",
    "\n",
    "    return (equalOdds + statParity)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_adult(numeric_only=True)\n",
    "X = adult.X.values\n",
    "y = adult.y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_model = DecisionTreeFair(max_depth=4)\n",
    "fair_model.fit(X_train, y_train, 3)\n",
    "fair_preds = fair_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8109602129256808\n",
      "-0.04143384402984229\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, fair_preds))\n",
    "xyz_fair = np.append(X_test, y_test[:,None], axis=1)\n",
    "xyz_fair = np.append(xyz_fair, np.array(fair_preds)[:,None], axis=1)\n",
    "print(avg_statParity_equalOdds(xyz_fair, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeScratch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\co41c\\OneDrive - ucsc.edu\\UCSC\\Sophmore\\XAI\\fair-decision-trees\\fair-decision-tree-scratch.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/co41c/OneDrive%20-%20ucsc.edu/UCSC/Sophmore/XAI/fair-decision-trees/fair-decision-tree-scratch.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m DecisionTreeScratch(max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/co41c/OneDrive%20-%20ucsc.edu/UCSC/Sophmore/XAI/fair-decision-trees/fair-decision-tree-scratch.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/co41c/OneDrive%20-%20ucsc.edu/UCSC/Sophmore/XAI/fair-decision-trees/fair-decision-tree-scratch.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionTreeScratch' is not defined"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeScratch(max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8242680679724289\n",
      "-0.20955129719890703\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds))\n",
    "xyz = np.append(X_test, y_test[:,None], axis=1)\n",
    "xyz = np.append(xyz, np.array(preds)[:,None], axis=1)\n",
    "print(avg_statParity_equalOdds(xyz, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289087558861666\n",
      "-0.20525036901417382\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "sklearn_preds = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, sklearn_preds))\n",
    "sklearn_xyz = np.append(X_test, y_test[:,None], axis=1)\n",
    "sklearn_xyz = np.append(sklearn_xyz, np.array(sklearn_preds)[:,None], axis=1)\n",
    "print(avg_statParity_equalOdds(sklearn_xyz, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d8e415af57141efce7fce2689149f05652dd0a0a2cbf826c959d0bbd7fadd8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
